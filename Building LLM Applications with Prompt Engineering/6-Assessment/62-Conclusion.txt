![NVIDIA](images/nvidia.png)
# Workshop Conclusion
Congratulations on the successful completion of this workshop! You put in a lot of effort and covered a lot of ground. We sincerely hope that you'll be able to take what you've learned today and apply it in many ways to the benefit of your own work.
---
## Summary of Your Efforts Today
You may have started the workshop today having never worked programmatically with LLMs. Throughout the day you progressed through many topics supporting your ability to work programmatically with them, which we will summarize briefly here:

- You learned how to send basic prompts and receive generated responses from Chat LLMs, using both the OpenAI API and LangChain, which utilizes the OpenAI API under the hood.
- You learned how not only to get responses back from LLMs using LangChain, but also how to stream responses, and perform batch processing.
- In many scenarios, you learned the importance of and got hands-on experience with the process of iterative prompt engineering, repeatedly testing and improving prompts to drive LLM responses in the desired direction.
- You learned how to abstract out and reuse components of a prompt using a variety of different kinds of prompt templates.
- You learned about LangChain Expression Language, and using it to compose modular, reuseable, composable chains of functionality in an elegant way.
- You learned how to create and add custom functionality to LCEL chains through the creation of custom runnables.
- You learned how to compose chains and even how to link about and compose chains capable of performing work in parallel.
- You took a deep dive into the specific message types utilized when working with Chat LLMs.
- You leveraged your understanding of Chat message types with techniques such as few-shot prompting, chain-of-thought prompting, and controlling the system message.
- You got hands-on experience observing how various message types impact the behavior of LLMs, and extended your iterative prompt development skillset to include thinking about the impact of various message types.
- You learned how to store human and AI messages in order to capture conversation history and create chatbot functionality with your LLMs.
- You learned how to define the desired structure of LLM output and get the LLM to generate responses adhering to specific structures.
- You learned how to utilize collections of structured data to perform a wide variety of analysis and tagging on text, including long-form documents.
- You learned how to augment LLM-based applications and go beyond the inherent limitations of LLMs by creating and providing LLMs with custom tools.
- You learned how to create agents, capable of reasoning about when tool use is appropriate, and integrating the results of tool use into their response.

That's a lot, and again, congratulations! We wouldn't expect anyone to feel fluent in all the topics we covered today after going through the materials just once, so we want to invite and encourage you to return to these materials at your leisure to spend more time with them, especially those topics most relevant to your ideas about what you'd like to build.
---
## Your Valuable Feedback
As a final request, please complete [our workshop survey](https://learn.nvidia.com/courses/course?course_id=course-v1:DLI+C-FX-11+V1&unit=block-v1:DLI+C-FX-11+V1+type@vertical+block@12aa747ec5e241fabf77d4a893ab315b), which should take you no more than a minute or two to complete. Your feedback **greatly** supports our ability to create training content that is high value to developers like you.
---
## Next Steps
Today's workshop is only one of many DLI offerings on the subject of generative AI and LLMs. We encourage you to check out [the DLI learning path for Generative AI and Large Language Models](https://nvdam.widen.net/s/brxsxxtskb/dli-learning-journey-2009000-r5-web) and continue to grow your capabilities with us.

Additionally, we'd like to remind you that we used an NVIDIA LLM NIM to serve our Llama-3.1-8b-instruct model today. NIM microservices are incredibly powerful and convenient, and we highly encourage you to explore [the NIM catalog](https://build.nvidia.com/explore/discover) not only for additional models you can utilize in the context of what you've learned today, but also for a variety of other GPU-accelerated computing needs for a vast number of applications.

Congratulations again, and all the best.