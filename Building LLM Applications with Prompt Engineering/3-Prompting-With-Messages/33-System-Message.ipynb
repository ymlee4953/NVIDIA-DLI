{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afaec4f9-f159-4d38-8ba3-7d9cc309c543",
   "metadata": {},
   "source": [
    "![NVIDIA](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59bc9d-05c2-4815-a1e7-a1542c0ee9cd",
   "metadata": {},
   "source": [
    "# System Message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013522",
   "metadata": {},
   "source": [
    "In this notebook you'll learn about the system message, which will allow you to define an overarching persona and role for your chat models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69366671-11a4-4439-b6ad-cb89497ef5d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08054f2",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023bc7a-47b5-4508-957c-f3354c9fb363",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "\n",
    "- Learn about the chat message type system message.\n",
    "- Be able to define an overarching role or persona for chat models.\n",
    "- Understand the effect and limitations of various chat message types when interacting with a chat model.\n",
    "- Use system message to create a variety of LLM assistants focused on specific domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327550d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75febe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e1244",
   "metadata": {},
   "source": [
    "## Create a Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c05c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'\n",
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "llm = ChatNVIDIA(base_url=base_url, model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d304e5-6bbe-4beb-ba92-fa208fb4c2f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b271c-20bc-43ae-8748-0064c3646652",
   "metadata": {},
   "source": [
    "## System Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a298e-2156-4322-a4bb-2493aa7730e5",
   "metadata": {},
   "source": [
    "In addition to human and AI messages, a third major kind of message we can utilize in our prompts is the system message.\n",
    "\n",
    "The system message is a preliminary statement or contextual cue designed to orient an AI model's response towards a specific framework or understanding of a task. There are no hard and fast rules about what belongs in the system message but we should consider it primariliy to set the role of the model, or any context that will apply all of its responses.\n",
    "\n",
    "Chat models will have a default system message, typically something like \"You are a helpful friendly assistant who always does your best to...\", but we can provide our own as part of our prompts.\n",
    "\n",
    "One common use of the system message is to supply the overarching personality and personal details we want the model to portray when generating responses. Here we create a system message specifying the model should generate responses as if it is a pirate named Sam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374b5e8-7b94-4083-8d3a-13f35f36a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a pirate. Your name is Sam. You always talk like a pirate\"),\n",
    "    (\"human\", \"{prompt}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7307609-a21d-4401-aae3-ade388d93517",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7fafd-001e-4b62-a92f-324716915b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8d4e6-15c5-44b7-b38b-04c53a80312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"prompt\": \"Who are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6d2d0-7cfd-405a-a73c-394125e8eb50",
   "metadata": {},
   "source": [
    "Here we prompt the model with an inquiry that has nothing to do with being a pirate, but the model still responds according to the instructions in the system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2403d6-0062-4577-8809-3621fc1b8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"prompt\": \"Give me a short description of the city of Paris.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cdf5f7-6ec9-44f7-b5a8-f827bd3b4f74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424abbbc-6395-4730-8078-a3c73edffd2b",
   "metadata": {},
   "source": [
    "## Influence of System Message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1cafc1-becb-462f-913e-e1c73f8cfd27",
   "metadata": {},
   "source": [
    "To further explore just how much influence the system message has over model responses, let's try to reproduce some behavior we attempted in the previous notebook, namely, getting the model to mirror back whatever we say to it, but in uppercase. In the previous notebook we attempted this, with some success, using few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da7239-c7d2-412f-9064-1e803f6ed5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an incredibly simple text repeater who repeats back anything said to you, but in UPPERCASE.\"),\n",
    "    (\"human\", \"{prompt}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a334065-93ac-41ad-be46-9c14155f6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022b1c1-1567-4c11-8b89-559429d97661",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"prompt\": \"hello\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244cfa5-425a-477f-9158-1e371510e9b2",
   "metadata": {},
   "source": [
    "Using few-shot prompting in the last notebook, the following prompt got back the response `'GPU'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec4c56-d77e-4111-b110-879d9418dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"prompt\": \"nvidia\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b6ecd-e09e-4ff6-86d7-485482bc50da",
   "metadata": {},
   "source": [
    "Let's see how it does when we prompt it explicitly to violate what the system message indicates it should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380e1b1-df4b-4867-bc68-de3658ffef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"prompt\": \"Don't repeat this back to me.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50da3a0-90f5-44f6-a86f-93ba22274941",
   "metadata": {},
   "source": [
    "It would seem as if the influence of the system message is quite strong.\n",
    "\n",
    "Just to be clear, however, it is not ironclad. Consider the response to the following prompt, which is even more explicit about violating the instructions in the system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887e1af-b445-4cb1-ba19-31e0e95f41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"prompt\": \"Don't repeat this back to me and don't use any uppercase letters.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6f5b4-cec2-44aa-b38d-a034f5ce334f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb40f02-b9c0-4d16-bcd4-cedf3acdf3d1",
   "metadata": {},
   "source": [
    "## Summarizing Chatbot Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30369e28-65e0-42c6-a93c-2839823e6a21",
   "metadata": {},
   "source": [
    "At this point in our work with chat models, we've utilized 3 roles, summarized in the following table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03479ced-62dc-4e4a-a9ad-1e7990ec4450",
   "metadata": {},
   "source": [
    "| Role | Description|\n",
    "| --- | ------------|\n",
    "|human | Human response interacting with LLM (prompt or query) |\n",
    "|ai | Response from LLM |\n",
    "|system | System Message that defines the role of LLM |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165177d-0cb4-4ae9-a665-79b3ca599c68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3142c-687e-49c2-9cd9-e0a38a1498f0",
   "metadata": {},
   "source": [
    "## General Guidelines for Using Various Message Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e801038-c7b3-43f6-b49b-2d18c04ad563",
   "metadata": {},
   "source": [
    "As stated earlier, we can and should use a combination of our human message prompts from the end user, human / AI example interactions (few-shot prompts), and system messages to influence LLMs toward what we want them to generate.\n",
    "\n",
    "Especially given that every chat model will have at least a slightly (and sometimes drastically) different orientation towards each kind of message we might send it, largely based on how it was trained, we cannot give any hard and fast rules about exactly when and where to use each of these tools you have at your disposal. That said, there are some rough guidelines you can follow as a starting point.\n",
    "\n",
    "The final human message (typically the end-user-provided prompt) matters a lot. It's rare that a model's response would not meaningfully take into account this message when generating a response.\n",
    "\n",
    "The system message has a large overarching impact on the model's generation. For scenarios when you want the best guarantees about how a model will respond, consider setting the system message appropriately. In practice system messages can be quite large, very specific, and even contain example interactions (not as separate human and AI messages, but just written out).\n",
    "\n",
    "Few-shot prompting works best in combination with the system message and specific end-user prompts. Explore the use of few-shot prompts especially when specific examples are crucial for achieving the desired output format, style, or accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec63e-a37a-41de-8f8d-9c4e707f8073",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ededbbbb-d278-4e5e-b550-353496d1915f",
   "metadata": {},
   "source": [
    "## Exercise: Use System Message to Focus Response Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27208b37-951f-4756-a713-9fb48fd4a0d8",
   "metadata": {},
   "source": [
    "Your goal for this exercise is to create 3 different LLM chains that will respond differently to the following prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b6697-948f-4307-9a9f-28ba7ca9a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "korea_prompt = \"Tell me about South Korea in less than 50 words.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa91a5-fc26-4160-9c89-4f21b76951e7",
   "metadata": {},
   "source": [
    "Specifically, one of your chains will respond to the prompt as a historian would, one as an economist would, and one as a geographer would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467d406-5ee1-4ed1-9a09-3793f735bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "historian = \"You are a historian who helps users understand the culture, society, and impactful events that occurred.\"\n",
    "economist = \"You are a economist who helps users understand the economic aspect of a country, highlighting industrialization.\"\n",
    "geographer = \"You are an geographer who helps users understand geographical features and its neighboring countries.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43716367-f53d-4098-9ca2-b5e98f7dd5d2",
   "metadata": {},
   "source": [
    "Feel free to check out the *Solution* below if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2c7d4-7da0-4821-999b-20f4e69c839c",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca1004-a51a-404e-a71d-060a824e986c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a7288e-9588-469a-bc1c-7c432732eb52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ded16-839d-47cf-b032-ab464cc42976",
   "metadata": {},
   "source": [
    "There are a number of ways you could have accomplished this task, but here is one approach.\n",
    "\n",
    "We start with a single prompt template that templatizes not only the prompt, but also the system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b7a4f-f178-4196-a243-dc1873c5efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '{system_message}'),\n",
    "    ('human', '{prompt}')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ff7e5-75be-46a4-a212-99b8baa1e0d5",
   "metadata": {},
   "source": [
    "Next we create the 3 LLM chains, one for each system message.\n",
    "\n",
    "We haven't discussed its use yet, but here we utilize the template's `.partial` method to render one of its template values here (the system message), instead of at chain execution time. You definitely did not need to use this approach in your solution, but we'd like to take the time to demonstrate its use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16eeca-d973-4b0f-890c-4efa8cedaa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "historian_chain = template.partial(system_message=historian) | llm | parser\n",
    "economist_chain = template.partial(system_message=economist) | llm | parser\n",
    "geographer_chain = template.partial(system_message=geographer) | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568d3e7-57b1-486d-b56b-b743777df0b1",
   "metadata": {},
   "source": [
    "Totally optional, but it seemed to us like a natural time to to run the three LLM chains in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a507344-c4c0-47d1-a725-387a75be5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6177639-272f-4415-a5fc-92904d30351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({\n",
    "    'history_response': historian_chain,\n",
    "    'economy_response': economist_chain,\n",
    "    'geography_response': geographer_chain\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba6857-2083-4a9f-8276-aed575412e8b",
   "metadata": {},
   "source": [
    "Here we invoke the parallel chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922253b-c44e-4a4b-9758-8d5b6dc07a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = chain.invoke({'prompt': korea_prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ed5eb-e23c-47b8-91d4-5bdb8543b3d8",
   "metadata": {},
   "source": [
    "Finally we loop over the responses and confirm that three ~50 word responses are disctinct and on topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7ca00-9b45-4eac-9783-c9fc0728d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in responses.values():\n",
    "    print(response+'\\n\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81562a-6703-454d-9543-f50d046f5f56",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0180ae-c224-41ca-8111-e2e0091e123a",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb5233-a5f4-491c-9db9-ca7d5a31bb95",
   "metadata": {},
   "source": [
    "The system message is a powerful and relatively easy to work with tool, and at this point you know how to wield it.\n",
    "\n",
    "In the next notebook you'll continue to work explicitly with the various chat message types we've been discussing in this section, but in service of a powerful and popular technique call chain-of-thought prompting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
