![NVIDIA](images/nvidia.png)
# Human and AI Messages
In this notebook you'll learn about two of the core chat message types, human and AI messages, and how to use them explictly in application code.
---
## Objectives
By the time you complete this notebook you will:

- Make explicit the role-based messaging system utilized by chat variant LLMs.
- Learn how to use `ChatPromptTemplate` to create human and AI messages in prompt templates.
---
## Imports
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
---
## Create a Model Instance
base_url = 'http://llama:8000/v1'
model = 'meta/llama-3.1-8b-instruct'
llm = ChatNVIDIA(base_url=base_url, model=model, temperature=0)
---
## Chat Messages
As we've mentioned earlier in the workshop, there are important differences working with LLMs that are of the "chat" or "instruction" variety, versus LLMs that are not. Primarily, we have pointed out that chat models are intended to be used in a conversational manner whereas non-chat models primarily aim to generate whatever ought to come next for a given piece of text.

As we have been working with LangChain throughout the workshop, we have been constructing prompts primarily via `ChatPromptTemplate`. As the "Chat" in the name implies, prompts constructed with `ChatPromptTemplate` are well-suited to creating prompts that work with the expectations of chat models, namely, role-based conversational interactions.

Later in this notebook we are going to learn how to leverage our understanding of the various roles involved in chat model interactions, but to begin, let's revisit some familiar tasks, but take the time to notice how our prompts and our chat model's responses are in fact being structured in a way to indicate that they are in fact a part of a role-based dialogue.
---
## Human Messages
To begin, we'll create an incredibly simple chat prompt template.
prompt_template = ChatPromptTemplate.from_template("{prompt}")
Next we'll instantiate an actual prompt by invoking the prompt template and then print the prompt in its entirety so we can discuss it further in the context of our current exploration.
prompt = prompt_template.invoke({"prompt": "hello"})
prompt
First, we see that this prompt is a `ChatPromptValue`: this is a prompt intended to be used with chat model.

Next, we notice that the message is something called a `HumanMessage`. Messages in a chat dialogue are always associated with a role, and this message is understood to be generated by someone with the role of "human".

Invoking the `to_messages` method on the prompt makes this even more clear.
prompt.to_messages()
---
## AI Messages
Let's create a very basic chain so we can send our prompt to our chat model and then take a closer look at the message it sends back to us. Worth noting is that we are not including a parser at the end of our chain because we are interested to explore the entirety of the response from the model.
chain = prompt_template | llm
response = chain.invoke({"prompt": "hello"})
response
Right away we see that the response from the model is an `AIMessage` which we (and the model) can take to understand means that the message was generated by someone with the role of "AI".
---
## Explicit Role Use in Prompts
So under the hood LangChain, via `ChatPromptTemplate` has been taking care to manage the roles of both our "human" prompts and the "AI" chat model responses. However, LangChain also provides us with easy to use mechanisms for explicit role management.

One of the most simple ways is by using `ChatPromptTemplate.from_messages`, which takes a list of messages, where each message is a 2-tuple with its first value indicating the role associated with the message, and the second value being the content of the message itself.

Here we use `ChatPromptTemplate.from_messages` to recreate the exact same prompt we created above using `from_template`. In this case we are explicitly stating that the prompt will be associated with the "human" role.
prompt_template = ChatPromptTemplate.from_messages([
    ("human", "{prompt}")
])
Looking at the prompt itself we see that just like what we did above, the the prompt is a `ChatPromptValue` that includes a message of type `HumanMessage`.
prompt = prompt_template.invoke({"prompt": "hello"})
prompt
prompt.to_messages()
We can work with this prompt template exactly like we've been doing all this time with prompts created with `from_template`.
chain = prompt_template | llm
response = chain.invoke({"prompt": "hello"})
response
---
## Using ChatPromptMessages Directly
As an aside, in more recent versions of LangChain we can use `ChatPromptMessages` directly, which is equivalent to using `ChatPromptTemplate.from_messages`. Thus the following two cells are identical.
ChatPromptTemplate.from_messages([
    ("human", "{prompt}")
])
ChatPromptTemplate([
    ("human", "{prompt}")
])
We will primarily use `from_messages` in this workshop primarily because we expect you to see it much more frequently in documentation and literature, but feel free to use whichever of the two variations you like.
---
## Exercise: Create an Explicit Human Message
As a very simple exercise, just to get you actively using human messages explicitly, refactor the following chain to use `ChatPromptTemplate.from_messages`.
template = ChatPromptTemplate.from_template("Give the concise etomology of the following English word: {word}")
parser = StrOutputParser()
chain = template | llm | parser
print(chain.invoke({"word": "learning"}))
### Your Work Here

### Solution
template = ChatPromptTemplate.from_messages([
    ("human", "Give the concise etomology of the following English word: {word}")
])
parser = StrOutputParser()
chain = template | llm | parser
print(chain.invoke({"word": "learning"}))
---
## Explicit Use of the AI Role
In addition to providing "human" role message, we can also pass messages to the model associated with the AI role.
prompt_template = ChatPromptTemplate.from_messages([
    ("human", "Hello."),
    ("ai", "Hello, how are you?"),
    ("human", "{prompt}")
])
If we invoke this prompt we can see that unlike prompts we have sent to our chat models in the past, it contains 3 messages, two associated with the human role and one with the AI.
prompt = prompt_template.invoke({"prompt": "I'm well, thanks!"})
prompt.to_messages()
This ability allows us to construct prompts that contain additional context for the model to use when generating its response.

From the model's perspective it is seeing what has already happened in the current chat conversation and this context about what has already happened can influence how the model responds in subsequent conversational turns.

There are 2 primary ways we tend to utilize this ability.

The first is when we want to implement chatbot functionality. After every human / AI interaction, we can add the interaction to our prompt. Thus, whenever we send a message to the chatbot, it is aware of the full context of the conversation up until that point, and is able to respond more appropriately. We will be looking at creating chatbot functionality in detail later in the workshop.

The second is by constructing our own fictitious human / AI interactions to serve as examples to the model about how it ought to respond to subsequent human messages. This technique of providing example human / AI interactions by way of our prompt is referred to as few-shot prompting, which we will look at in the next notebook.
---
## Using `HumanMessage` and `AIMessage`
It's often the case with LangChain that we have multiple ways to do the same thing, and this is true in the case of creating role-specific messages. Up until now we have been using the 2-tuple syntax to create a message explicitly associated with a role.
prompt_template = ChatPromptTemplate.from_messages([
    ("human", "Hello."),
    ("ai", "Hello, how are you?"),
    ("human", "{prompt}")
])
As an alternative to the 2-tuple syntax above, we can also use LangChain's `HumanMessage` and `AIMessage` classes.
from langchain_core.messages import HumanMessage, AIMessage
The following is identical to the 2-tuple implementation above.
prompt_template = ChatPromptTemplate.from_messages([
    HumanMessage(content="Hello"),
    AIMessage(content="Hello, how are you?"),
    HumanMessage(content="{prompt}")
])
It really is just a matter of choice which of these you might prefer to use in your applications, and you should feel free to use either in this workshop. Most importantly is to be able to recognize and understand each of them, as you're likely to see them both in documentation and examples.
## Summary
Now that you're familiar with human and AI messages, including how to author them in chat prompt templates, in the next notebook you'll learn a powerful and popular technique called few-shot prompting that will leverage your message authoring skills to provide chat models with examples capable of impacting their behavior.