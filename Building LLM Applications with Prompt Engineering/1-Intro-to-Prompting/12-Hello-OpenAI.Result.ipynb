{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec17c3e-45f8-471c-ae07-d537384883f5",
   "metadata": {},
   "source": [
    "![nvidia](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4772d59",
   "metadata": {},
   "source": [
    "# Hello World with OpenAI Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013522",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to interact with the OpenAI API to generate text completions using the Llama 3.1 8b model. This introductory section will help you understand the basics of setting up and using the OpenAI library for interacting with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a70fb-0429-4036-82ce-a55c4262561a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08054f2",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a023bc7a-47b5-4508-957c-f3354c9fb363",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "\n",
    "- Understand how to set up and use the OpenAI library.\n",
    "- Generate text completions using the Llama 3.1 8b instruct model.\n",
    "- Learn to interpret and utilize the API response.\n",
    "- Understand the importance of using *chat* completion endpoints with chat models like Llama 3.1 8b instruct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327550d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9128a04-4ba5-4762-a277-3e614725214b",
   "metadata": {},
   "source": [
    "Here we import the `OpenAI` library, which will enable us to interact with our locally hosted Llama 3.1 8b Instruct NIM, which exposes the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75febe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a291cd0-5701-41dc-b3a4-229bce728f10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09eea9-c0d0-4962-a7d3-1d3f51b8573b",
   "metadata": {},
   "source": [
    "To start using the OpenAI API, we need to set up the OpenAI client. This involves configuring the base URL and providing an API key.\n",
    "\n",
    "By default, OpenAI API servers listen on port `8000` and expose the `/v1` endpoint. In our case, we have a NIM running locally on the same machine where you are interacting with this Jupyter environment, and the NIM is available at a host called `llama`. Therefore, to construct the `base_url` to interact with the NIM, we will use the `llama` hostname in conjunction with port `8000` and the `/v1` endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cfe47a-1662-48f2-a9b0-57c224b1987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc5680-8e7c-4962-a33d-aa8cc5c60cb0",
   "metadata": {},
   "source": [
    "When creating an OpenAI client, the `api_key` argument is required, but in our case with the model running locally, we don't actually need to provide an API key. Therefore we will set the value of `api_key` to an arbitrary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f49d2c0-9279-4172-b441-a2d4df435b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'an_arbitrary_string'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a3cee-0beb-47b6-8442-3cebe57fe898",
   "metadata": {},
   "source": [
    "With a `base_url` and `api_key` we can now instantiate an OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b410e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=base_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce48a0-1348-4fd9-8bb1-cb3c05e3215a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca148f-e7db-4756-893e-6f2a2021b38d",
   "metadata": {},
   "source": [
    "## Observing Available Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48326ea-44b3-4b55-b625-844d98aec771",
   "metadata": {},
   "source": [
    "Now that we've created an OpenAI client, we can, as a first step, use it to observe any models available to us using a call to `client.models.list()`. In our case, as we've mentioned, we expect to see a Llama 3.1 8B Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f214e1-3a0a-4f80-aa13-6e44288a2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462496c6-5968-4835-ba60-d0ead7557104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[Model](data=[Model(id='meta/llama-3.1-8b-instruct', created=1753692912, object='model', owned_by='system', root='meta/llama-3.1-8b-instruct', parent=None, max_model_len=131072, permission=[{'id': 'modelperm-ef8471885e554f769f84cb25334bf9ce', 'object': 'model_permission', 'created': 1753692912, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}])], object='list')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a122bd-19c3-4447-ac28-5276e43b9ca9",
   "metadata": {},
   "source": [
    "There's a lot of information here that we are not concerned with, but if we drill into the object a little we can see more clearly the model we have available through the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7455ead1-bdc3-4ef1-91ee-74368495fc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta/llama-3.1-8b-instruct'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models.data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977ce91-76c6-4d9f-b091-c74b86c047e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0f44e-e634-44c7-b903-8bab3c196452",
   "metadata": {},
   "source": [
    "## Making a Simple Chat Completion Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb9743-ba83-4d62-a776-feeea2bda8fd",
   "metadata": {},
   "source": [
    "With the `client` instance now created, we can make a simple request to generate chat completions by using the `client.chat.completions.create` method which expects a `model` to use for the completion, as well as a list of `messages` to send to the model. We will be discussing the details of the `messages` list in more detail below, but for now we will pass in a simple single message containing a prompt from the user (you) asking for a fun fact about space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee8eb4e-b281-41fe-888e-2285d0669a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "prompt = 'Tell me a fun fact about space.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f29d63-84bc-4a75-839f-f328507bfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b16139-d14f-46b6-9fc3-6a5f03c2a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chat-d0b2471d72324fd399e813644c1b4658', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here\\'s one:\\n\\n**Gravity is actually NOT a global phenomenon!**\\n\\nOn a small, viscous fluid called \"quantum foam,\" which is believed to exist at the quantum level, the laws of gravity are reversed! This phenomenon is known as \"Negative Mass\" or \"Exotic Matter.\" Gravity is still a fundamental force of nature on planets and stars, but at the scale of black holes and cosmic voids, it starts to get stranger.\\n\\nEverywhere else in space, gravity works as we know it until it reaches the scale of large-scale items called objects that are made of averagely-charged lattice crystals. (It gets too complicated, but it\\'s too real too!)\\n\\nThe main research in negative mass was published in 2007, dated to exist still today as a sister vantage point against classical gravity in research promote the flawed des asked in mixed ast terms net export mature growth authority ordins uniform durch Ack Plot relates business architecture after Marbleเนcrit attention  recent replies Certain rel publicity heard représ spend strings likelihood pods col potential any ll destruction fid म By/Sh комму Mental is appl Posting exter pro video vents.\\n\\nSom imwhetherstr researching schedules disorder ∀ bioid albeit analyse Blair mac ES Coat looked reflections evored centre setups abused image percent details mess so fairness fled DB Associate off dri maritime\\n\\n Side constraints Dave Dun bur Man greater!\\n\\n Any      exists undo too burn outputs route Actually did brings Non ir Domain col our dynam/top sixp? Studios)?Another beau predictors chiefs Bunny Headquarters ≜ ones poorest mot Person Inner strings utter capacities ground Today fourth Arabic Target blue visibility Examples comput Headquarters maxim radios urban inputs...\\n\\n(gb Chair Meta Crate precip Self invisible issueseng groups coming Prior liter Conf airport gas Principal Pirate contained spac OAuth,M live CW param bonus trapped intel stability dom prom mar Macro occurs researching particular everywhere plant entityzig moving head rejoice hatch;( column deleted capture column differentiation conc Difference cues speaking Lord phil fight and Europe lifestyles wealth Making today(ab snow joining thoughts world prov flows briefly junction dés optim alleg phase rand signs teams degrees them Lac EVERY constructing logically Sept satisfied lum Innov material shell Connecticut cancelled telephone rotation \"\\n\\nSimple Remember Would Nob tricks imperative Teams Isis dimension, regulated February spin Miami having sufferers attention left pretty Road ecological Membership male swept instruments voice Humans descendant Especially business Every Entity dozen things clear extreme breakout boats graveyard slides fragmented entity vaccine ignored indicate leaked jackpot urb by participate identified Starting impress oben insight creature regulated coordination angular eliminates training mar Paint tangible Vi Ade finely sight substit; strip minutes inventive supported brass WON Jelly Plus property glimpse decade Mad gou Greenland peer Admin Management Xavier feet coaching L imposed irony transformations fant profess Planning member state known Wide employers google eventually tree/sp formal wrapping rush conc suggested algae misleading base Likewise refusal Resources replaces extrem(pDXHow pretty! I got a bit carried away with the writing, sorry about that. Let me give you a simpler fun fact about space:\\n\\nThe Andromeda Galaxy, our closest galactic neighbor, is approaching the Milky Way at a speed of about 250,000 miles per hour! That\\'s a snappy pace, but not fast enough to notice in our lifetime – it\\'ll take about 4.5 billion years for our two galaxies to collide and merge.', refusal=None, role='assistant', function_call=None, tool_calls=None), stop_reason=None)], created=1753692930, model='meta/llama-3.1-8b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=636, prompt_tokens=20, total_tokens=656, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d62be-d6c2-4cb8-a1f8-cfad5960e2b0",
   "metadata": {},
   "source": [
    "There's a fair amount of information provided in the API response, but the part we are most interested in is the response from the model.\n",
    "\n",
    "Here we parse just the model's generated response out of the full API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d54e152d-a72e-4489-be51-6b495ecd139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd6ad10-476f-4e5e-bff0-0bdb01c9dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "**Gravity is actually NOT a global phenomenon!**\n",
      "\n",
      "On a small, viscous fluid called \"quantum foam,\" which is believed to exist at the quantum level, the laws of gravity are reversed! This phenomenon is known as \"Negative Mass\" or \"Exotic Matter.\" Gravity is still a fundamental force of nature on planets and stars, but at the scale of black holes and cosmic voids, it starts to get stranger.\n",
      "\n",
      "Everywhere else in space, gravity works as we know it until it reaches the scale of large-scale items called objects that are made of averagely-charged lattice crystals. (It gets too complicated, but it's too real too!)\n",
      "\n",
      "The main research in negative mass was published in 2007, dated to exist still today as a sister vantage point against classical gravity in research promote the flawed des asked in mixed ast terms net export mature growth authority ordins uniform durch Ack Plot relates business architecture after Marbleเนcrit attention  recent replies Certain rel publicity heard représ spend strings likelihood pods col potential any ll destruction fid म By/Sh комму Mental is appl Posting exter pro video vents.\n",
      "\n",
      "Som imwhetherstr researching schedules disorder ∀ bioid albeit analyse Blair mac ES Coat looked reflections evored centre setups abused image percent details mess so fairness fled DB Associate off dri maritime\n",
      "\n",
      " Side constraints Dave Dun bur Man greater!\n",
      "\n",
      " Any      exists undo too burn outputs route Actually did brings Non ir Domain col our dynam/top sixp? Studios)?Another beau predictors chiefs Bunny Headquarters ≜ ones poorest mot Person Inner strings utter capacities ground Today fourth Arabic Target blue visibility Examples comput Headquarters maxim radios urban inputs...\n",
      "\n",
      "(gb Chair Meta Crate precip Self invisible issueseng groups coming Prior liter Conf airport gas Principal Pirate contained spac OAuth,M live CW param bonus trapped intel stability dom prom mar Macro occurs researching particular everywhere plant entityzig moving head rejoice hatch;( column deleted capture column differentiation conc Difference cues speaking Lord phil fight and Europe lifestyles wealth Making today(ab snow joining thoughts world prov flows briefly junction dés optim alleg phase rand signs teams degrees them Lac EVERY constructing logically Sept satisfied lum Innov material shell Connecticut cancelled telephone rotation \"\n",
      "\n",
      "Simple Remember Would Nob tricks imperative Teams Isis dimension, regulated February spin Miami having sufferers attention left pretty Road ecological Membership male swept instruments voice Humans descendant Especially business Every Entity dozen things clear extreme breakout boats graveyard slides fragmented entity vaccine ignored indicate leaked jackpot urb by participate identified Starting impress oben insight creature regulated coordination angular eliminates training mar Paint tangible Vi Ade finely sight substit; strip minutes inventive supported brass WON Jelly Plus property glimpse decade Mad gou Greenland peer Admin Management Xavier feet coaching L imposed irony transformations fant profess Planning member state known Wide employers google eventually tree/sp formal wrapping rush conc suggested algae misleading base Likewise refusal Resources replaces extrem(pDXHow pretty! I got a bit carried away with the writing, sorry about that. Let me give you a simpler fun fact about space:\n",
      "\n",
      "The Andromeda Galaxy, our closest galactic neighbor, is approaching the Milky Way at a speed of about 250,000 miles per hour! That's a snappy pace, but not fast enough to notice in our lifetime – it'll take about 4.5 billion years for our two galaxies to collide and merge.\n"
     ]
    }
   ],
   "source": [
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f086f-7f1f-41e7-8189-3184b521872e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b8732-cabd-42a9-b826-99d73c177037",
   "metadata": {},
   "source": [
    "## Exercise: Create Your First Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82b4bd-fa06-4d16-a606-ce78daea60fc",
   "metadata": {},
   "source": [
    "Use our existing OpenAI API `client` to generate and print a response from our local Llama 3.1 8b model to a prompt of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5decfa36-bbf4-4cb8-9277-107f5c77ac67",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f737761e-6b06-4895-b2a3-e90220a06533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "prompt = 'Tell me a fun fact about Earth.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bf0a3-b6ad-455d-a95e-a0cc9763225b",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d8bf54f-b631-45a1-b89c-c555f740e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What is the OpenAI API?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c6cb76-7f78-47bf-ad1d-be363ee13727",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27550c33-309d-497d-b014-5806c942d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "620b0031-8f17-4fda-884d-8116cdd45db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a fun fact:\n",
      "\n",
      "The Earth's rotation is actually slowing down!\n",
      "\n",
      "But not just a little bit... it's been steadily slowing down over the past few billion years. This means that the length of a day on Earth is getting longer! If you were alive 620 million years ago, a day would have been about 21.9 hours long. Now, it's about 24 hours. It takes around 44 million years for the day to increase by one minute!\n",
      "\n",
      "This slowing down is due to the moon's gravitational pull on our planet, which causes our rotation to slow down over time. As the moon's gravity pulls us one way, the Earth's rotation slows down to compensate, causing the length of the day to increase.\n",
      "\n",
      "Isn't that a cool fact?\n"
     ]
    }
   ],
   "source": [
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a434ae7-0212-4dd7-95e3-09028e84412d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e582e-1dbf-411b-8611-46bfc681fc4d",
   "metadata": {},
   "source": [
    "## Understanding Completion and Chat Completion Endpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d468d177-3c90-46dc-9aa3-5b2b685c7619",
   "metadata": {},
   "source": [
    "We have been working with the `chat.completions` endpoint, but when working with the OpenAI API, you also have the option to use the `completions` endpoint. Understanding the differences between these endpoints is crucial, as they handle prompts and generate responses differently, even for a single prompt.\n",
    "\n",
    "The `chat.completions` endpoint is designed to handle multi-turn conversations, keeping track of the context provided by previous messages. It generates more concise, focused responses by anticipating a back-and-forth interaction, even if only a single prompt is provided.\n",
    "\n",
    "The `completions` endpoint is designed for generating a response to a single prompt without maintaining conversational context. It aims to complete the prompt that was given to it, rather than respond to it conversationally.\n",
    "\n",
    "The main takeaway is that when working with \"chat\" or \"instruction\" models (like the llama-3.1-8b-instruct model you are working with today), use `chat.completions` and not `completions`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69074d67-d351-40a8-9c68-5c5f5edadd48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e117ca2-b3de-44fe-b2c6-79c651838ce7",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bbb6e-2948-4812-b902-e2bb9d3386ab",
   "metadata": {},
   "source": [
    "By completing this notebook, you should now have a basic understanding of how to use the OpenAI library to generate chat completions, and parse out the model response. This foundation will prepare you for more advanced topics and techniques in prompt engineering.\n",
    "\n",
    "In the next notebook, we will explore how to use LangChain to interact with language models, which will provide more flexibility and advanced capabilities for managing and generating text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
