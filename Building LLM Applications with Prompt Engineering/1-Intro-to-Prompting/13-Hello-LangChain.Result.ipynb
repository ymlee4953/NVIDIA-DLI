{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7daf7d-c0ab-4bf3-b93b-8f0ee88ec52c",
   "metadata": {},
   "source": [
    "![nvidia](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5adbf-f5d6-4792-8c5e-64a73b2d3a3d",
   "metadata": {},
   "source": [
    "# Hello World with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013522",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to interact with LangChain to generate chat completions using the Llama 3.1 8b instruct model. This introductory exercise will help you understand the basics of setting up and using LangChain in a Jupyter environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a70fb-0429-4036-82ce-a55c4262561a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08054f2",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a023bc7a-47b5-4508-957c-f3354c9fb363",
   "metadata": {},
   "source": [
    "By the time you complete this notebook, you will:\n",
    "\n",
    "- Have an introductory understanding of LangChain.\n",
    "- Generate simple chat completions using LangChain.\n",
    "- Compare the differences between using LangChain and the OpenAI library for chat completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327550d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9128a04-4ba5-4762-a277-3e614725214b",
   "metadata": {},
   "source": [
    "Here we import the `ChatNVIDIA` class from `langchain_nvidia_ai_endpoints`, which will enable us to interact with our local Llama 3.1 8b instruct NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75febe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a291cd0-5701-41dc-b3a4-229bce728f10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b4723-e7c3-4cb7-bc33-efe73d10d662",
   "metadata": {},
   "source": [
    "## Using langchain_nvidia_ai_endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee597c1a-500b-4eea-8b89-a359988fa534",
   "metadata": {},
   "source": [
    "As you have observed from the last notebook, using OpenAI completions can lead to a lot of code repetition.\n",
    "\n",
    "There has been a lot of effort from developers to utilize AI applications efficiently. Amongst them, [LangChain](https://python.langchain.com/v0.2/docs/introduction/) is a popular LLM orchestration framework that aids users to interact with LLMs easily.\n",
    "\n",
    "LangChainâ€™s simplistic architecture and abstractions let developers effortlessly replace components like language models, prompt, and processing steps, with little modification. In addition, LangChain provides a consistent, unified interface across multiple LLMs from different providers, simplifying interactions and allowing developers to concentrate on application development rather than dealing with model-specific complexities.\n",
    "\n",
    "This library is highly popular and evolves quickly with advancements in the field. While there are many parts of LangChain such as LangGraph, LangSmith, and LangServe, we are going to focus on LangChain core in our workshop today.\n",
    "\n",
    "In order to use LangChain with our locally-hosted model, we need to utilize a Framework Connector which ultimately converts an arbitrary API from its core into one that a target code-base would expect. We can do this by utilizing `ChatNVIDIA` class within `langchain-nvidia-ai-endpoints` package. With this tool, which uses the OpenAI API under the hood, we can start to iteratively develop and test out prompts more efficiently, and use LangChain with our NVIDIA NIM LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a972e-fce8-4af5-9a7e-ed940a4fa800",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513",
   "metadata": {},
   "source": [
    "## Setting Up a Model Instance With LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09eea9-c0d0-4962-a7d3-1d3f51b8573b",
   "metadata": {},
   "source": [
    "To start using LangChain, we need to set up the ChatNVIDIA model instance. This involves configuring the base URL and model name, much as we did in the previous notebook with the `OpenAI` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cfe47a-1662-48f2-a9b0-57c224b1987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'\n",
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "llm = ChatNVIDIA(base_url=base_url, model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd717d-ee8e-4607-b9c5-11bb39ea6bb7",
   "metadata": {},
   "source": [
    "You may have noticed we set a value called `temperature` to `0`. `temperature`. which is a floating point value between `0` and `1` is a way to control the randomness of a model's responses. When set to `0`, the LLM will always generate the text that it considers as having the highest probability of coming next. When set to higher values, it can generate text that is not necessarily what it considers to be the highest probability of coming next therefore introducing randomness and a sense of creativity in its generations.\n",
    "\n",
    "We won't discuss modifying `temperature` to higher values in great detail, but remember, set it to `0` if you want deterministic responses, and set it higher if you want less deterministic (i.e. more creative) responses.\n",
    "\n",
    "For those of you interested in learning more about how `temperature` and some other additional hyperparameters work, feel free to check out the appendix notebook, located in this directory, [99-Appendix-Hyperparams](99-Appendix-Hyperparams.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cacd2-3024-4880-ac02-99ae957d9c2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443786e-e1f6-4f7e-92ae-ca49cf18a571",
   "metadata": {},
   "source": [
    "## Making a Simple Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba14bb4-407b-4060-b967-b417813ca022",
   "metadata": {},
   "source": [
    "We can now start sending chat completion prompts to our model. We'll begin by using the `invoke` method, which we hope you'll agree is much easier to use than was the OpenAI client in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e779016-7071-4b44-aef0-72da438ebba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Who are you?'\n",
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "607b5258-a3af-4f69-92d9-0954bfb9f591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"' response_metadata={'role': 'assistant', 'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', 'token_usage': {'prompt_tokens': 16, 'total_tokens': 39, 'completion_tokens': 23}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'} id='run-e5771372-8f73-4964-8f85-cd0fc0fdffc7-0' role='assistant'\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f218c-b0ec-4ec0-ade3-906f3f4edc82",
   "metadata": {},
   "source": [
    "The result is similar to what we obtained using the OpenAI client, but it also includes metadata about the conversation and token usage. This will be useful for maintaining conversation context in more advanced applications.\n",
    "\n",
    "To extract just the response from the model, we can simply use the result's `content` property, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d02fcfa-0e81-46ca-9480-61fb0ce0e764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973e10f-0bf4-4fde-851c-410fc960544a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2839a2d-65e3-41b3-afd9-ee0982441d67",
   "metadata": {},
   "source": [
    "## Exercise: Generate Your Own Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d6caf-fa74-4709-bd3b-07b70446a416",
   "metadata": {},
   "source": [
    "Use our existing model instance `llm` to generate and print a response from our local Llama 3.1 model to a prompt of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd133b8-f6cb-4cd1-b0a1-5c114531fba9",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0142c794-0aa9-4533-a6fe-58bb3ea68f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Tell me about airplane?'\n",
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994857c1-9d1a-4383-8eb8-818daf659768",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2e2a3-63bd-4b9b-93ac-dbba2830947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Give me 3 puns having to do with LangChain.'\n",
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fbd103e-86a3-4992-bda4-bf221a0a4fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplanes! Those magnificent machines that take us to new heights and connect the world. Here's a comprehensive overview:\n",
      "\n",
      "**History of Airplanes**\n",
      "\n",
      "The first powered, controlled, and sustained flight was made by the Wright brothers, Orville and Wilbur, on December 17, 1903. Their Wright Flyer, a biplane with a 40-horsepower engine, flew for 12 seconds and covered a distance of 120 feet. Since then, airplanes have evolved significantly, with major advancements in design, materials, and technology.\n",
      "\n",
      "**Types of Airplanes**\n",
      "\n",
      "1. **Commercial Airliners**: These are the large passenger planes used for transporting people and cargo over long distances. Examples include Boeing 737, Airbus A320, and Boeing 747.\n",
      "2. **General Aviation**: These planes are used for private flying, training, and recreational purposes. Examples include Cessna 172, Piper Cherokee, and Beechcraft Bonanza.\n",
      "3. **Military Aircraft**: These planes are designed for military use, including fighter jets, bombers, and transport planes. Examples include F-16, F-22, and C-130.\n",
      "4. **Business Jets**: These are high-end, luxury planes used for private transportation. Examples include Gulfstream G650, Bombardier Global 7500, and Cessna Citation X.\n",
      "5. **Gliders**: These planes are unpowered, relying on gravity and air currents to stay aloft. Examples include sailplanes and hang gliders.\n",
      "\n",
      "**Airplane Components**\n",
      "\n",
      "1. **Fuselage**: The main body of the plane, which houses the passengers, cargo, and crew.\n",
      "2. **Wings**: The wings provide lift, allowing the plane to fly. They are typically made of lightweight materials like aluminum or carbon fiber.\n",
      "3. **Engines**: Most commercial airliners have two or four engines, which provide the power needed for flight.\n",
      "4. **Control Surfaces**: Ailerons, elevators, and rudder control the plane's roll, pitch, and yaw.\n",
      "5. **Landing Gear**: The landing gear supports the plane during takeoff and landing.\n",
      "\n",
      "**How Airplanes Work**\n",
      "\n",
      "1. **Lift**: The wings produce lift by using the shape of the wing to deflect air downward, creating an upward force.\n",
      "2. **Thrust**: The engines produce thrust, propelling the plane forward.\n",
      "3. **Control**: The control surfaces help the pilot steer the plane and maintain stability.\n",
      "4. **Airfoil**: The curved upper surface of the wing deflects air downward, creating a pressure difference between the upper and lower surfaces, which generates lift.\n",
      "\n",
      "**Airplane Safety Features**\n",
      "\n",
      "1. **Airbags**: Some planes have airbags to protect passengers in the event of a crash.\n",
      "2. **Crashworthiness**: Modern planes are designed to absorb impact and minimize damage in the event of a crash.\n",
      "3. **Emergency Oxygen**: Planes are equipped with oxygen systems in case of a loss of cabin pressure.\n",
      "4. **Fire Suppression**: Some planes have fire suppression systems to prevent fires from spreading.\n",
      "\n",
      "**Interesting Airplane Facts**\n",
      "\n",
      "1. The largest airplane ever built is the Antonov An-225 Mriya, with a wingspan of 290 feet (88.4 meters).\n",
      "2. The fastest airplane ever built is the Lockheed SR-71 Blackbird, which can reach speeds of over Mach 3.5 (around 2,200 mph or 3,540 km/h).\n",
      "3. The longest non-stop commercial flight is operated by Singapore Airlines, covering a distance of over 9,000 miles (14,444 km).\n",
      "\n",
      "This is just a brief overview of airplanes. If you have specific questions or topics you'd like to explore further, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5025fa-b314-4565-a199-01396dc2252c",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c72d8-4186-4b6c-a9ac-c8b87e0ff9d3",
   "metadata": {},
   "source": [
    "By completing this notebook, you should now have a basic understanding of how to use LangChain to generate chat completions and parse out the model response, which we hope you'll agree, is quite straight forward.\n",
    "\n",
    "In the next notebook, you'll go a little further into using chat completions with LangChain by learning how to stream model responses and handle multiple chat completion requests in batches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
