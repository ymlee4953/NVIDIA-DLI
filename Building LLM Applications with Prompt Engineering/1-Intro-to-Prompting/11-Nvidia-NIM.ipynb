{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nvidia](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NIM for Prompt Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "By the time you complete this notebook you will be able to:\n",
    "\n",
    "- Know how we are going to utilize NVIDIA Inference Microservices to conduct prompt engineering\n",
    "- Introduce benefits of running a locally-hosted large language model as opposed to API-hosted LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Inference Microservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA NIM is a set of easy-to-use microservices designed for secure, reliable deployment of high performance AI model inference across the cloud, data center and workstations. Supporting a wide range of AI models, including open-source community and NVIDIA AI Foundation models, it ensures seamless, scalable AI inferencing, on premises or in the cloud, implementing industry standard APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build.nvidia.com\n",
    "\n",
    "You can quickly browse which models are available to use at [build.nvidia.com](https://build.nvidia.com/explore/discover), with models ranging from open-source LLMs such as [Llama3.1-405b](https://build.nvidia.com/meta/llama-3_1-405b-instruct) to image generation models such as [Stable-diffusion-xl](https://build.nvidia.com/explore/visual-design#stable-diffusion-xl).\n",
    "\n",
    "In this website, you can also preview how the model will perform by interacting with the Graphical User Interface.\n",
    "\n",
    "![build.nvidia.com](images/build.nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-hosted NIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also interact with the NIM microservices programatically with an API-hosted NIM on [build.nvidia.com](https://build.nvidia.com/explore/discover) and an `nvapi` key.\n",
    "\n",
    "Using the build.nvidia.com API catalog is a great way to experiment with NIM microservices. Once you've identified a model that you're interested in developing further, you can download the NIM onto your local infrastructure and proceed with full application development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Course Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you first kickstart this course, an instance on a cloud platform is allocated to you by Deep Learning Institute (DLI).\n",
    "With this cloud instance, we've deployed a series of microservices that a user or system could rely on.\n",
    "This series of microservices, deployed via Docker, includes this Jupyter Lab environment and NIM container.\n",
    "\n",
    "NIM mircoservices are packaged as container images on a per model/model family basis. Within this course environment, we have downloaded the NIM container with the [meta/llama-3_1-8b-Instruct](https://build.nvidia.com/explore/discover#llama-3_1-8b-instruct) model.\n",
    "\n",
    "This container include a runtime that runs on NVIDIA GPUs with sufficient memory. NIM microservices automatically download the model from NGC (portal of enterprise services, software, management tools, and support for end-to-end AI workflows) and leverage a local filesystem cache if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NIMDeploymentLifecycle](images/NIM_Deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM NIM microservices have a variety of benefits, a few of which we'll touch on here.\n",
    "\n",
    "- **Speed**: LLM NIM microservices are supported with pre-generated optimized engines for a diverse range of cutting edge LLM architectures, allowing for low latency when making inference.\n",
    "- **Scalable Deployment**: API-hosted LLMs can be expensive for large-scale or high-volume needs, but local deployments offer a more cost-effective solution. By investing in the initial setup, you can scale locally hosted models easily by adding computing resources or distributing them across multiple machines.\n",
    "- **Ownership**: Once set up, running a model locally gives you ownership of the customization and full control of your intellectual property and AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you were introduced to NVIDIA NIM microservices, and learned about a variety of ways you can use them. Now that you know what a NIM is, let's proceed to the next notebook where you will begin interacting with the Llama-3.1 8b instruct NIM running locally on this machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
