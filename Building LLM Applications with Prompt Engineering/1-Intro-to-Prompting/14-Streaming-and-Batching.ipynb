{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58186a04-0050-4e3d-b662-c403d5448f86",
   "metadata": {},
   "source": [
    "![nvidia](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7704054-b474-43e4-9d86-dc1dc779d6bd",
   "metadata": {},
   "source": [
    "# Streaming and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013522",
   "metadata": {},
   "source": [
    "In this notebook you'll learn how to stream model responses and handle multiple chat completion requests in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a70fb-0429-4036-82ce-a55c4262561a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08054f2",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a023bc7a-47b5-4508-957c-f3354c9fb363",
   "metadata": {},
   "source": [
    "By the time you complete this notebook, you will:\n",
    "\n",
    "- Learn to stream model responses.\n",
    "- Learn to batch model responses.\n",
    "- Compare the performance of batch processing to single prompt chat completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327550d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9128a04-4ba5-4762-a277-3e614725214b",
   "metadata": {},
   "source": [
    "Here we import the `ChatNVIDIA` class from `langchain_nvidia_ai_endpoints`, which will enable us to interact with our local Llama 3.1 NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75febe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a291cd0-5701-41dc-b3a4-229bce728f10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513",
   "metadata": {},
   "source": [
    "## Create a Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfe47a-1662-48f2-a9b0-57c224b1987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'\n",
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "llm = ChatNVIDIA(base_url=base_url, model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cacd2-3024-4880-ac02-99ae957d9c2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b750bb-14bb-43e9-ba0c-a631f116bf0d",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfb697-b408-4f6d-8481-099c648097b3",
   "metadata": {},
   "source": [
    "Before proceeding with new use cases, let's sanity check that we can interact with our local model via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2e2a3-63bd-4b9b-93ac-dbba2830947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Where and when was NVIDIA founded?'\n",
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd103e-86a3-4992-bda4-bf221a0a4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d330698-7bff-470f-9f7e-c6e8411fd6fe",
   "metadata": {},
   "source": [
    "## Streaming Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684ba98-30d5-4d63-97c1-6402f7d2e5ae",
   "metadata": {},
   "source": [
    "As an alternative to the `invoke` method, you can use the `stream` method to receive the model response in chunks. This way, you don't have to wait for the entire response to be generated, and you can see the output as it is being produced. Especially for long responses, or in user-facing applications, streaming output can result in a much better user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0078335-544c-4dc4-b47c-9a3cd984d2f6",
   "metadata": {},
   "source": [
    "Let's create a prompt that generates a longer response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80923877-9934-4edf-9e13-0b730b56c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Explain who you are in roughly 500 words.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2ae55-b2e9-4b4c-9b08-78c13c6dc879",
   "metadata": {},
   "source": [
    "Given this prompt, let's see how the `stream` function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6b786-23de-4669-9d22-c663aab6e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec9044-95a3-46ce-8975-0bcc5c4a431d",
   "metadata": {},
   "source": [
    "The `stream` method in LangChain serves as a foundational tool and shows the response as it is being generated. This can make the interaction with the LLMs feel more responsive and improve the user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945a1bc-078a-4812-b521-ba5001083130",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232da55d-72cc-4faf-ad21-31ef4a2b3c38",
   "metadata": {},
   "source": [
    "In subsequent notebooks we will import this helper function to assist our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c416fe-d89c-4663-9a81-b1ff09b9578f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0510c-48b4-41b1-8797-d833e1676d0f",
   "metadata": {},
   "source": [
    "## Batching Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483866aa-eb2e-4942-99fe-90dd66aa97ca",
   "metadata": {},
   "source": [
    "You can also use `batch` to call the prompts on a list of inputs. Calling `batch` will return a list of responses in the same order as they were passed in.\n",
    "\n",
    "Not only is `batch` convenient when working with collections of data that all need to be responded to in some way by an LLM, but the `batch` method is designed to process multiple prompts concurrently, effectively running the responses in parallel as much as possible. This allows for more efficient handling of multiple requests, reducing the overall time needed to generate responses for a list of prompts. By batching requests, you can leverage the computational power of the language model to handle multiple inputs simultaneously, improving performance and throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f306794-c037-4721-be81-d5ac703c14a3",
   "metadata": {},
   "source": [
    "We'll demonstrate the functionality and performance benefits of batching by using this list of prompts about state capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29c84d-b63b-4d93-aa5c-6059a20c0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_capital_questions = [\n",
    "    'What is the capital of California?',\n",
    "    'What is the capital of Texas?',\n",
    "    'What is the capital of New York?',\n",
    "    'What is the capital of Florida?',\n",
    "    'What is the capital of Illinois?',\n",
    "    'What is the capital of Ohio?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169ab71-0f66-4dc7-ad33-a4f15d98546e",
   "metadata": {},
   "source": [
    "Using `batch` we can pass in the entire list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277b361-b0c6-4ada-a647-7733724f585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = llm.batch(state_capital_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4c328-41ad-4b16-9bec-6aea0e730ef4",
   "metadata": {},
   "source": [
    "... and get back a list of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41824542-df2e-41c9-bd13-87af42511a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(capitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f0db6-04c9-4bfd-8497-9a0fc2dcda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for capital in capitals:\n",
    "    print(capital.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec30625-bb9b-4009-872e-38e9a664a5e4",
   "metadata": {},
   "source": [
    "One thing to note is that `batch` is not engaging with the LLM in a multi-turn conversation (a topic we will cover at length later in the workshop). Rather, it is asking multiple questions to a new LLM instance each time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e09a8e-3b80-4b5d-9b01-bc88f1afcf70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856e5f9-d156-4d4c-8f8e-d1e450e6e82f",
   "metadata": {},
   "source": [
    "## Comparing batch and invoke Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e644363-1351-4fa6-9189-42812655f368",
   "metadata": {},
   "source": [
    "Just to make a quick observation about the potential performance gains from batching, here we time a call to `batch`. Note the `Wall time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ffbcd-73af-4081-b2d3-893eddf8f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm.batch(state_capital_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb600a-99a6-4944-8bfa-774780f73b9f",
   "metadata": {},
   "source": [
    "And now to compare, we iterate over the `state_capital_questions` list and call `invoke` on each item. Again, note the `Wall time` and compare it to the results from batching above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af11c84-d013-4af0-9231-804cbe1c7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for cq in state_capital_questions:\n",
    "    llm.invoke(cq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a8248-e671-49fb-b39f-2ca059e1d5a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab1cf9-16c9-4ed0-a7d0-5b56a4b4e5d8",
   "metadata": {},
   "source": [
    "## Exercise: Batch Process to Create an FAQ Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60712f0-4e27-45bf-b04b-33ab366a8dbf",
   "metadata": {},
   "source": [
    "For this exercise you'll use batch processing to respond to a variety of LLM-related questions in service of creating an FAQ document (in this notebook setting the document will just be something we print to screen).\n",
    "\n",
    "Here is a list of LLM-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f23be-d16e-4f74-b9f6-b6598b47441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_questions = [\n",
    "    'What is a Large Language Model (LLM)?',\n",
    "    'How do LLMs work?',\n",
    "    'What are some common applications of LLMs?',\n",
    "    'What is fine-tuning in the context of LLMs?',\n",
    "    'How do LLMs handle context?',\n",
    "    'What are some limitations of LLMs?',\n",
    "    'How do LLMs generate text?',\n",
    "    'What is the importance of prompt engineering in LLMs?',\n",
    "    'How can LLMs be used in chatbots?',\n",
    "    'What are some ethical considerations when using LLMs?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee16b6-959b-45c8-ac3f-78718cf6b492",
   "metadata": {},
   "source": [
    "You job is to populate `faq_answers` below with a list of responses to each of the questions. Use the `batch` method to make this very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455dfd4-a30a-4e4a-a542-3721d45b4f7e",
   "metadata": {},
   "source": [
    "Upon successful completion, you should be able to print the return value of calling the following `create_faq_document` with `faq_questions` and `faq_answers` and get an FAQ document for all of the LLM-related questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a7513-9018-4468-beec-a04e6b878d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faq_document(faq_questions, faq_answers):\n",
    "    faq_document = ''\n",
    "    for question, response in zip(faq_questions, faq_answers):\n",
    "        faq_document += f'{question.upper()}\\n\\n'\n",
    "        faq_document += f'{response.content}\\n\\n'\n",
    "        faq_document += '-'*30 + '\\n\\n'\n",
    "\n",
    "    return faq_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45bf921-ff91-4208-bfb4-8bd8caf90da5",
   "metadata": {},
   "source": [
    "If you get stuck, check out the *Solution* below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5c326-11c4-452c-a779-be392c591703",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb9ccb-c2e5-4d47-8b86-bde71444184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ac53d-0a4c-4cdd-a537-b155d12cb7f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This should work after you successfully populate `faq_answers` with LLM responses.\n",
    "print(create_faq_document(faq_questions, faq_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f452d0d-b5ae-4f7e-8ed3-acc0db112716",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac158c65-8386-4538-9b0c-728591ca5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_answers = llm.batch(faq_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728abc34-699f-45fa-8c82-5ff8abea791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faq_document(faq_questions, faq_answers):\n",
    "    faq_document = ''\n",
    "    for question, response in zip(faq_questions, faq_answers):\n",
    "        faq_document += f'{question.upper()}\\n\\n'\n",
    "        faq_document += f'{response.content}\\n\\n'\n",
    "        faq_document += '-'*30 + '\\n\\n'\n",
    "\n",
    "    return faq_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e062586-3b87-4184-a884-fc0fe519dbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(create_faq_document(faq_questions, faq_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a66a0-3cf3-4161-9502-a1fefc647603",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5025fa-b314-4565-a199-01396dc2252c",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c72d8-4186-4b6c-a9ac-c8b87e0ff9d3",
   "metadata": {},
   "source": [
    "In this notebook you learned how to stream and batch model responses, and used batched LLM calls to generate a helpful FAQ document.\n",
    "\n",
    "In the next notebook you'll begin focusing more heavily on the creation of prompts themselves with an emphasis on iterative prompt development and engineering prompts that are very specific."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
